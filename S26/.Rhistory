x <- c(232320,
640361,
640869,
640312,
640548,
640533,
640537,
640328,
641154,
650189,
644342,
644144,
640416,
640992,
687544,
360067,
644163,
644205,
644943,
680255,
644728,
644184,
680195,
685133,
680255,
644943,
650189,
644205,
644144,
640992,
640361,
644342,
687544,
640869,
640312,
360067,
640416,
640533,
641154,
640548,
680195,
687517,
640537,
650260,
621263,
640328,
685228 )
table(x)
unique(680255
644943
unique(680255
644943
unique(x)
t(unique(x))
1688*0.9
1688*0.1
121217.10 - 95439.31 + 24430
8130.71+1549.79
24430-25909.94
232587-16068
257017-33552.13
121217.10 + 24430 - 33552.13
121217.10 + 24430 - 33552.13
121217.10 + 24430.0 - 33552.13
112095 - 95439.31
104561.44+24430-35329.19
1*3 + 3:2 + 4*1 + 6*1
1*3 + 3*2 + 4*1 + 6*1
1*1 + 2*2 + 3*1 + 4*2 + 5*1
1*2 + 3*1 + 5*4
1*1 + 3*1 + 4*2 + 6*3
1*1 + 2*2 + 3*1 + 5*1 + 6*2
2*2 + 3*2 + 4*1 + 5*1 + 6*1
579.94*0.6
579.94*0.4
580*0.6
580*0.4
0.4*580+0.52*1600
0.6*580+0.48*1600
1-pf(6, 24, 18)
240+0.48*1600
0.4*580
0.4*580
0.48*1600
240+0.48*1600
240+0.52*1600
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Load required libraries
library(tidyverse)
library(broom)
library(knitr)
library(ggplot2)
library(dagitty)
library(ggdag)
library(patchwork)
# Set seed for reproducibility
set.seed(42)
# Define the DAG structure
dag <- dagify(
C ~ A + B,
T ~ A + C,
Y ~ B + C + T,
coords = list(
x = c(A = 1, B = 3, C = 2, T = 1, Y = 3),
y = c(A = 3, B = 3, C = 2, T = 1, Y = 1)
)
)
# Plot the DAG
ggdag(dag, node_size = 20, text_size = 4) +
theme_dag_blank() +
geom_dag_edges() +  # Remove the _arc version
geom_dag_node(aes(color = name), size = 20) +
geom_dag_text(color = "white", size = 4) +
scale_color_manual(values = c("A" = "lightcoral", "B" = "lightblue",
"C" = "lightblue", "T" = "lightgreen",
"Y" = "lightblue")) +
theme(legend.position = "none")
# Sample size
n <- 15000
# Generate confounders
A <- rnorm(n, 0, 1)
B <- rbinom(n, 1, 0.5)
# Generate treatment (influenced by A - this creates confounding)
latent_T <- 1.3 * A + 0.3 * A^2 + rnorm(n, 0, 1.4)
p_T <- 1 / (1 + exp(-latent_T))
T <- rbinom(n, 1, p_T)
# Generate mediator C (influenced by A, B, and T)
C <- 1.2 * A + 0.4 * B + 0.6 * T + 0.2 * A * T + 0.1 * A^2 + rnorm(n, 0, 1.2)
# Generate outcome Y (influenced by everything, including interactions)
Y <- 1.3 * C + 0.5 * B + 0.4 * T + 0.4 * C * T + 0.5 * C^2 + rnorm(n, 0, 3.5)
# Create dataframe
df <- data.frame(A = A, B = B, T = T, C = C, Y = Y)
# Show first few rows
head(df)
simulate_true_ate <- function(n_sims = 1000, sample_size = 10000) {
ate_estimates <- numeric(n_sims)
for(i in 1:n_sims) {
# Generate new sample with same structure
A_sim <- rnorm(sample_size, 0, 1)
B_sim <- rbinom(sample_size, 1, 0.5)
eps_C <- rnorm(sample_size, 0, 1.2)
eps_Y <- rnorm(sample_size, 0, 3.5)
# Potential outcomes under treatment (T=1)
C_1 <- 1.2 * A_sim + 0.4 * B_sim + 0.6 * 1 + 0.2 * A_sim * 1 + 0.1 * A_sim^2 + eps_C
Y_1 <- 1.3 * C_1 + 0.5 * B_sim + 0.4 * 1 + 0.4 * C_1 * 1 + 0.5 * C_1^2 + eps_Y
# Potential outcomes under control (T=0)
C_0 <- 1.2 * A_sim + 0.4 * B_sim + 0.6 * 0 + 0.2 * A_sim * 0 + 0.1 * A_sim^2 + eps_C
Y_0 <- 1.3 * C_0 + 0.5 * B_sim + 0.4 * 0 + 0.4 * C_0 * 0 + 0.5 * C_0^2 + eps_Y
# Average treatment effect for this simulation
ate_estimates[i] <- mean(Y_1 - Y_0)
}
return(ate_estimates)
}
cat("Computing true Average Treatment Effect via simulation...\n")
true_ate_sims <- simulate_true_ate()
true_ate <- mean(true_ate_sims)
ate_ci <- quantile(true_ate_sims, c(0.025, 0.975))
cat(sprintf("True ATE: %.3f\n", true_ate))
cat(sprintf("95%% CI: [%.3f, %.3f]\n", ate_ci[1], ate_ci[2]))
# Function to estimate ATE from a fitted model
estimate_ate <- function(model, data) {
# Create counterfactual datasets
data_t1 <- data_t0 <- data
data_t1$T <- 1
data_t0$T <- 0
# Predict under both scenarios
y1_pred <- predict(model, data_t1)
y0_pred <- predict(model, data_t0)
# Return average difference
mean(y1_pred - y0_pred)
}
# Define different model specifications
models <- list(
"T only" = "Y ~ T",
"T + B" = "Y ~ T + B",
"T + A" = "Y ~ T + A",
"T + A + B" = "Y ~ T + A + B",
"T + C" = "Y ~ T + C",
"T + B + C" = "Y ~ T + B + C",
"T + A + C" = "Y ~ T + A + C",
"T + A + B + C" = "Y ~ T + A + B + C"
)
# Fit models and compute results
results <- data.frame()
for(i in 1:length(models)) {
model_name <- names(models)[i]
formula_str <- models[[i]]
# Fit the model
model <- lm(as.formula(formula_str), data = df)
# Get model performance metrics
model_summary <- glance(model)
r_squared <- model_summary$r.squared
# Estimate ATE
ate_estimate <- estimate_ate(model, df)
# Bootstrap confidence interval for ATE
n_boot <- 200
boot_ates <- numeric(n_boot)
for(b in 1:n_boot) {
boot_indices <- sample(nrow(df), replace = TRUE)
boot_data <- df[boot_indices, ]
boot_model <- lm(as.formula(formula_str), data = boot_data)
boot_ates[b] <- estimate_ate(boot_model, boot_data)
}
ate_ci_lower <- quantile(boot_ates, 0.025)
ate_ci_upper <- quantile(boot_ates, 0.975)
# Check if CI contains true ATE
covers_true <- ate_ci_lower <= true_ate & true_ate <= ate_ci_upper
# Store results
results <- rbind(results, data.frame(
Model = model_name,
R_squared = r_squared,
ATE_estimate = ate_estimate,
CI_lower = ate_ci_lower,
CI_upper = ate_ci_upper,
Covers_true_ATE = covers_true,
stringsAsFactors = FALSE
))
}
# Sort by R-squared (predictive performance)
results_by_prediction <- results[order(-results$R_squared), ]
# Format for nice display
results_display <- results_by_prediction
results_display[2:5] <- round(results_display[2:5], 3)
kable(results_display,
caption = "Models Ranked by Predictive Performance (RÂ²)",
row.names = FALSE)
# Find best predictive model
best_pred_model <- results_by_prediction$Model[1]
best_pred_r2 <- results_by_prediction$R_squared[1]
best_pred_ate <- results_by_prediction$ATE_estimate[1]
best_pred_covers <- results_by_prediction$Covers_true_ATE[1]
# Find models that correctly estimate ATE
correct_models <- results[results$Covers_true_ATE == TRUE, ]
correct_models <- correct_models[order(-correct_models$R_squared), ]
cat("ðŸŽ¯ BEST PREDICTIVE MODEL:", best_pred_model, "\n")
cat("   - RÂ² =", round(best_pred_r2, 3), "(highest predictive accuracy)\n")
cat("   - ATE estimate =", round(best_pred_ate, 3), "(true ATE =", round(true_ate, 3), ")\n")
cat("   - Captures true causal effect?", ifelse(best_pred_covers, "âœ… YES", "âŒ NO"), "\n\n")
if(nrow(correct_models) > 0) {
best_causal_model <- correct_models$Model[1]
best_causal_r2 <- correct_models$R_squared[1]
best_causal_ate <- correct_models$ATE_estimate[1]
cat("ðŸ”¬ BEST CAUSAL MODEL:", best_causal_model, "\n")
cat("   - RÂ² =", round(best_causal_r2, 3), "\n")
cat("   - ATE estimate =", round(best_causal_ate, 3), "\n")
cat("   - Difference in RÂ² =", round(best_pred_r2 - best_causal_r2, 3), "\n")
} else {
cat("ðŸ”¬ NO MODELS correctly captured the true causal effect!\n")
}
# Define the DAG structure
dag <- dagify(
C ~ A + B,
T ~ A + C,
Y ~ B + C + T,
coords = list(
x = c(A = 1, B = 3, C = 2, T = 1, Y = 3),
y = c(A = 3, B = 3, C = 2, T = 1, Y = 1)
)
)
# Plot the DAG
ggdag(dag, node_size = 20, text_size = 4) +
theme_dag_blank() +
geom_dag_edges() +  # Remove the _arc version
geom_dag_node(aes(color = name), size = 20) +
geom_dag_text(color = "white", size = 4) +
scale_color_manual(values = c("A" = "lightcoral", "B" = "lightblue",
"C" = "lightblue", "T" = "lightgreen",
"Y" = "lightblue")) +
theme(legend.position = "none")
source("~/Documents/GitHub/CS6140/S26/build_site.R")
120852+40861
219301+81715
10*90+12*20+8*20+11*50+6*50+3*60+8*50+2*40+2*60+2*20+3*50
source("~/Documents/GitHub/CS6140/S26/build_site.R")
source("~/Documents/GitHub/CS6140/S26/build_site.R")
source("~/Documents/GitHub/CS6140/S26/build_site.R")
source("~/Documents/GitHub/CS6140/S26/build_site.R")
source("~/Documents/GitHub/CS6140/S26/build_site.R")
1/0.01
1/100
(610-2*38.27)*0.6
(610-2*38.27)*0.4
(610-2*38.27)*0.6 - (610-2*38.27)*0.4
(630.28-2*38.27)*0.6 - (610-2*38.27)*0.4
(630.28-2*38.27)*0.6
(630.28-2*38.27)*0.4
(630.28-2*38.27)*0.2
